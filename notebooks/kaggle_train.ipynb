{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PREPARATION\n",
    "import json\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Image.Image, int]:\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row[\"filename\"]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = row[\"label\"]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class GeoDataModule(L.LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 32) -> None:\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        df = self._create_unified_dataframe()\n",
    "        train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        self.train_dataset = CustomImageDataset(train_df)\n",
    "        self.val_dataset = CustomImageDataset(val_df)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "\n",
    "    def _create_unified_dataframe(self) -> pd.DataFrame:\n",
    "        street_location_dataset_path = f\"/kaggle/input/street-location-images/data\"\n",
    "        geolocation_dataset_path = f\"/kaggle/input/geolocation-geoguessr-images-50k/compressed_dataset\"\n",
    "        label_mapping_path = f\"/kaggle/input/street-location-images/country_to_index.json\"\n",
    "\n",
    "        with open(label_mapping_path, \"r\") as f:\n",
    "            label_mapping = json.load(f)\n",
    "\n",
    "        data = []\n",
    "        for img_name in os.listdir(street_location_dataset_path):\n",
    "            if img_name.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                img_path = os.path.join(street_location_dataset_path, img_name)\n",
    "                label_name = (img_name.rsplit(\".\", 1)[0] + \".json\")\n",
    "                label_path = os.path.join(street_location_dataset_path, label_name)\n",
    "                with open(label_path, \"r\") as f:\n",
    "                    label_data = json.load(f)\n",
    "                    label_str = label_data[\"country_name\"]\n",
    "                    label = label_mapping[label_str]\n",
    "                data.append([img_path, label])\n",
    "\n",
    "        for label_str in os.listdir(geolocation_dataset_path):\n",
    "            for img_name in os.listdir(f\"{geolocation_dataset_path}/{label_str}\"):\n",
    "                label = label_mapping.get(label_str)\n",
    "                if label:\n",
    "                    data.append([f\"{geolocation_dataset_path}/{label_str}/{img_name}\", label])\n",
    "\n",
    "        return pd.DataFrame(data, columns=[\"filename\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TINY VIT\n",
    "\n",
    "import itertools\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import timm\n",
    "from timm.models.layers import DropPath as TimmDropPath,\\\n",
    "    to_2tuple, trunc_normal_\n",
    "try:\n",
    "    # timm.__version__ >= \"0.6\"\n",
    "    from timm.models._builder import build_model_with_cfg\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    # timm.__version__ < \"0.6\"\n",
    "    from timm.models.helpers import build_model_with_cfg\n",
    "\n",
    "\n",
    "class Conv2d_BN(torch.nn.Sequential):\n",
    "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
    "                 groups=1, bn_weight_init=1):\n",
    "        super().__init__()\n",
    "        self.add_module('c', torch.nn.Conv2d(\n",
    "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
    "        bn = torch.nn.BatchNorm2d(b)\n",
    "        torch.nn.init.constant_(bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(bn.bias, 0)\n",
    "        self.add_module('bn', bn)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        c, bn = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = c.weight * w[:, None, None, None]\n",
    "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
    "            (bn.running_var + bn.eps)**0.5\n",
    "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
    "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m\n",
    "\n",
    "\n",
    "class DropPath(TimmDropPath):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__(drop_prob=drop_prob)\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def __repr__(self):\n",
    "        msg = super().__repr__()\n",
    "        msg += f'(drop_prob={self.drop_prob})'\n",
    "        return msg\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_chans, embed_dim, resolution, activation):\n",
    "        super().__init__()\n",
    "        img_size: Tuple[int, int] = to_2tuple(resolution)\n",
    "        self.patches_resolution = (img_size[0] // 4, img_size[1] // 4)\n",
    "        self.num_patches = self.patches_resolution[0] * \\\n",
    "            self.patches_resolution[1]\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        n = embed_dim\n",
    "        self.seq = nn.Sequential(\n",
    "            Conv2d_BN(in_chans, n // 2, 3, 2, 1),\n",
    "            activation(),\n",
    "            Conv2d_BN(n // 2, n, 3, 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, expand_ratio,\n",
    "                 activation, drop_path):\n",
    "        super().__init__()\n",
    "        self.in_chans = in_chans\n",
    "        self.hidden_chans = int(in_chans * expand_ratio)\n",
    "        self.out_chans = out_chans\n",
    "\n",
    "        self.conv1 = Conv2d_BN(in_chans, self.hidden_chans, ks=1)\n",
    "        self.act1 = activation()\n",
    "\n",
    "        self.conv2 = Conv2d_BN(self.hidden_chans, self.hidden_chans,\n",
    "                               ks=3, stride=1, pad=1, groups=self.hidden_chans)\n",
    "        self.act2 = activation()\n",
    "\n",
    "        self.conv3 = Conv2d_BN(\n",
    "            self.hidden_chans, out_chans, ks=1, bn_weight_init=0.0)\n",
    "        self.act3 = activation()\n",
    "\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.drop_path(x)\n",
    "\n",
    "        x += shortcut\n",
    "        x = self.act3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, out_dim, activation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        self.act = activation()\n",
    "        self.conv1 = Conv2d_BN(dim, out_dim, 1, 1, 0)\n",
    "        self.conv2 = Conv2d_BN(out_dim, out_dim, 3, 2, 1, groups=out_dim)\n",
    "        self.conv3 = Conv2d_BN(out_dim, out_dim, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 3:\n",
    "            H, W = self.input_resolution\n",
    "            B = len(x)\n",
    "            # (B, C, H, W)\n",
    "            x = x.view(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, depth,\n",
    "                 activation,\n",
    "                 drop_path=0., downsample=None, use_checkpoint=False,\n",
    "                 out_dim=None,\n",
    "                 conv_expand_ratio=4.,\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MBConv(dim, dim, conv_expand_ratio, activation,\n",
    "                   drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                   )\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(\n",
    "                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None,\n",
    "                 out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.norm = nn.LayerNorm(in_features)\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, dim, key_dim, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=(14, 14),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        # (h, w)\n",
    "        assert isinstance(resolution, tuple) and len(resolution) == 2\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = key_dim ** -0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.nh_kd = nh_kd = key_dim * num_heads\n",
    "        self.d = int(attn_ratio * key_dim)\n",
    "        self.dh = int(attn_ratio * key_dim) * num_heads\n",
    "        self.attn_ratio = attn_ratio\n",
    "        h = self.dh + nh_kd * 2\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.qkv = nn.Linear(dim, h)\n",
    "        self.proj = nn.Linear(self.dh, dim)\n",
    "\n",
    "        points = list(itertools.product(\n",
    "            range(resolution[0]), range(resolution[1])))\n",
    "        N = len(points)\n",
    "        attention_offsets = {}\n",
    "        idxs = []\n",
    "        for p1 in points:\n",
    "            for p2 in points:\n",
    "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
    "                if offset not in attention_offsets:\n",
    "                    attention_offsets[offset] = len(attention_offsets)\n",
    "                idxs.append(attention_offsets[offset])\n",
    "        self.attention_biases = torch.nn.Parameter(\n",
    "            torch.zeros(num_heads, len(attention_offsets)))\n",
    "        self.register_buffer('attention_bias_idxs',\n",
    "                             torch.LongTensor(idxs).view(N, N),\n",
    "                             persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and hasattr(self, 'ab'):\n",
    "            del self.ab\n",
    "        else:\n",
    "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
    "\n",
    "    def forward(self, x):  # x (B,N,C)\n",
    "        B, N, _ = x.shape\n",
    "\n",
    "        # Normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        # (B, N, num_heads, d)\n",
    "        q, k, v = qkv.view(B, N, self.num_heads, -\n",
    "                           1).split([self.key_dim, self.key_dim, self.d], dim=3)\n",
    "        # (B, num_heads, N, d)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (\n",
    "            (q @ k.transpose(-2, -1)) * self.scale\n",
    "            +\n",
    "            (self.attention_biases[:, self.attention_bias_idxs]\n",
    "             if self.training else self.ab)\n",
    "        )\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, self.dh)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyViTBlock(nn.Module):\n",
    "    r\"\"\" TinyViT Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int, int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        local_conv_size (int): the kernel size of the convolution between\n",
    "                               Attention and MLP. Default: 3\n",
    "        activation: the activation function. Default: nn.GELU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7,\n",
    "                 mlp_ratio=4., drop=0., drop_path=0.,\n",
    "                 local_conv_size=3,\n",
    "                 activation=nn.GELU,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        assert window_size > 0, 'window_size must be greater than 0'\n",
    "        self.window_size = window_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        assert dim % num_heads == 0, 'dim must be divisible by num_heads'\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        window_resolution = (window_size, window_size)\n",
    "        self.attn = Attention(dim, head_dim, num_heads,\n",
    "                              attn_ratio=1, resolution=window_resolution)\n",
    "\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        mlp_activation = activation\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=mlp_activation, drop=drop)\n",
    "\n",
    "        pad = local_conv_size // 2\n",
    "        self.local_conv = Conv2d_BN(\n",
    "            dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        res_x = x\n",
    "        if H == self.window_size and W == self.window_size:\n",
    "            x = self.attn(x)\n",
    "        else:\n",
    "            x = x.view(B, H, W, C)\n",
    "            pad_b = (self.window_size - H %\n",
    "                     self.window_size) % self.window_size\n",
    "            pad_r = (self.window_size - W %\n",
    "                     self.window_size) % self.window_size\n",
    "            padding = pad_b > 0 or pad_r > 0\n",
    "\n",
    "            if padding:\n",
    "                x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n",
    "\n",
    "            pH, pW = H + pad_b, W + pad_r\n",
    "            nH = pH // self.window_size\n",
    "            nW = pW // self.window_size\n",
    "            # window partition\n",
    "            x = x.view(B, nH, self.window_size, nW, self.window_size, C).transpose(2, 3).reshape(\n",
    "                B * nH * nW, self.window_size * self.window_size, C\n",
    "            )\n",
    "            x = self.attn(x)\n",
    "            # window reverse\n",
    "            x = x.view(B, nH, nW, self.window_size, self.window_size,\n",
    "                       C).transpose(2, 3).reshape(B, pH, pW, C)\n",
    "\n",
    "            if padding:\n",
    "                x = x[:, :H, :W].contiguous()\n",
    "\n",
    "            x = x.view(B, L, C)\n",
    "\n",
    "        x = res_x + self.drop_path(x)\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, C, H, W)\n",
    "        x = self.local_conv(x)\n",
    "        x = x.view(B, C, L).transpose(1, 2)\n",
    "\n",
    "        x = x + self.drop_path(self.mlp(x))\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic TinyViT layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        local_conv_size: the kernel size of the depthwise convolution between attention and MLP. Default: 3\n",
    "        activation: the activation function. Default: nn.GELU\n",
    "        out_dim: the output dimension of the layer. Default: dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., drop=0.,\n",
    "                 drop_path=0., downsample=None, use_checkpoint=False,\n",
    "                 local_conv_size=3,\n",
    "                 activation=nn.GELU,\n",
    "                 out_dim=None,\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TinyViTBlock(dim=dim, input_resolution=input_resolution,\n",
    "                         num_heads=num_heads, window_size=window_size,\n",
    "                         mlp_ratio=mlp_ratio,\n",
    "                         drop=drop,\n",
    "                         drop_path=drop_path[i] if isinstance(\n",
    "                             drop_path, list) else drop_path,\n",
    "                         local_conv_size=local_conv_size,\n",
    "                         activation=activation,\n",
    "                         )\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(\n",
    "                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "\n",
    "class TinyViT(nn.Module):\n",
    "    def __init__(self, img_size=224, in_chans=3, num_classes=1000,\n",
    "                 embed_dims=[96, 192, 384, 768], depths=[2, 2, 6, 2],\n",
    "                 num_heads=[3, 6, 12, 24],\n",
    "                 window_sizes=[7, 7, 14, 7],\n",
    "                 mlp_ratio=4.,\n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 use_checkpoint=False,\n",
    "                 mbconv_expand_ratio=4.0,\n",
    "                 local_conv_size=3,\n",
    "                 layer_lr_decay=1.0,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "        self.num_layers = len(depths)\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        activation = nn.GELU\n",
    "\n",
    "        self.patch_embed = PatchEmbed(in_chans=in_chans,\n",
    "                                      embed_dim=embed_dims[0],\n",
    "                                      resolution=img_size,\n",
    "                                      activation=activation)\n",
    "\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate,\n",
    "                                                sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            kwargs = dict(dim=embed_dims[i_layer],\n",
    "                          input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                            patches_resolution[1] // (2 ** i_layer)),\n",
    "                          depth=depths[i_layer],\n",
    "                          drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                          downsample=PatchMerging if (\n",
    "                              i_layer < self.num_layers - 1) else None,\n",
    "                          use_checkpoint=use_checkpoint,\n",
    "                          out_dim=embed_dims[min(\n",
    "                              i_layer + 1, len(embed_dims) - 1)],\n",
    "                          activation=activation,\n",
    "                          )\n",
    "            if i_layer == 0:\n",
    "                layer = ConvLayer(\n",
    "                    conv_expand_ratio=mbconv_expand_ratio,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            else:\n",
    "                layer = BasicLayer(\n",
    "                    num_heads=num_heads[i_layer],\n",
    "                    window_size=window_sizes[i_layer],\n",
    "                    mlp_ratio=self.mlp_ratio,\n",
    "                    drop=drop_rate,\n",
    "                    local_conv_size=local_conv_size,\n",
    "                    **kwargs)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # Classifier head\n",
    "        self.norm_head = nn.LayerNorm(embed_dims[-1])\n",
    "        self.head = nn.Linear(\n",
    "            embed_dims[-1], num_classes) if num_classes > 0 else torch.nn.Identity()\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "        self.set_layer_lr_decay(layer_lr_decay)\n",
    "\n",
    "    def set_layer_lr_decay(self, layer_lr_decay):\n",
    "        decay_rate = layer_lr_decay\n",
    "\n",
    "        # layers -> blocks (depth)\n",
    "        depth = sum(self.depths)\n",
    "        lr_scales = [decay_rate ** (depth - i - 1) for i in range(depth)]\n",
    "\n",
    "        def _set_lr_scale(m, scale):\n",
    "            for p in m.parameters():\n",
    "                p.lr_scale = scale\n",
    "\n",
    "        self.patch_embed.apply(lambda x: _set_lr_scale(x, lr_scales[0]))\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            for block in layer.blocks:\n",
    "                block.apply(lambda x: _set_lr_scale(x, lr_scales[i]))\n",
    "                i += 1\n",
    "            if layer.downsample is not None:\n",
    "                layer.downsample.apply(\n",
    "                    lambda x: _set_lr_scale(x, lr_scales[i - 1]))\n",
    "        assert i == depth\n",
    "        for m in [self.norm_head, self.head]:\n",
    "            m.apply(lambda x: _set_lr_scale(x, lr_scales[-1]))\n",
    "\n",
    "        for k, p in self.named_parameters():\n",
    "            p.param_name = k\n",
    "\n",
    "        def _check_lr_scale(m):\n",
    "            for p in m.parameters():\n",
    "                assert hasattr(p, 'lr_scale'), p.param_name\n",
    "\n",
    "        self.apply(_check_lr_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'attention_biases'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # x: (N, C, H, W)\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        x = self.layers[0](x)\n",
    "        start_i = 1\n",
    "\n",
    "        for i in range(start_i, len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            x = layer(x)\n",
    "\n",
    "        x = x.mean(1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.norm_head(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "_checkpoint_url_format = \\\n",
    "    'https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/{}.pth'\n",
    "\n",
    "\n",
    "def _create_tiny_vit(variant, pretrained=False, **kwargs):\n",
    "    # pretrained_type: 22kto1k_distill, 1k, 22k_distill\n",
    "    pretrained_type = kwargs.pop('pretrained_type', '22kto1k_distill')\n",
    "    assert pretrained_type in ['22kto1k_distill', '1k', '22k_distill'], \\\n",
    "        'pretrained_type should be one of 22kto1k_distill, 1k, 22k_distill'\n",
    "\n",
    "    img_size = kwargs.get('img_size', 224)\n",
    "    if img_size != 224:\n",
    "        pretrained_type = pretrained_type.replace('_', f'_{img_size}_')\n",
    "\n",
    "    num_classes_pretrained = 21841 if \\\n",
    "        pretrained_type  == '22k_distill' else 1000\n",
    "\n",
    "    variant_without_img_size = '_'.join(variant.split('_')[:-1])\n",
    "    cfg = dict(\n",
    "        url=_checkpoint_url_format.format(\n",
    "            f'{variant_without_img_size}_{pretrained_type}'),\n",
    "        num_classes=num_classes_pretrained,\n",
    "        classifier='head',\n",
    "    )\n",
    "\n",
    "    def _pretrained_filter_fn(state_dict):\n",
    "        state_dict = state_dict['model']\n",
    "        # filter out attention_bias_idxs\n",
    "        state_dict = {k: v for k, v in state_dict.items() if \\\n",
    "            not k.endswith('attention_bias_idxs')}\n",
    "        return state_dict\n",
    "\n",
    "    if timm.__version__ >= \"0.6\":\n",
    "        return build_model_with_cfg(\n",
    "            TinyViT, variant, pretrained,\n",
    "            pretrained_cfg=cfg,\n",
    "            pretrained_filter_fn=_pretrained_filter_fn,\n",
    "            **kwargs)\n",
    "    else:\n",
    "        return build_model_with_cfg(\n",
    "            TinyViT, variant, pretrained,\n",
    "            default_cfg=cfg,\n",
    "            pretrained_filter_fn=_pretrained_filter_fn,\n",
    "            **kwargs)\n",
    "\n",
    "\n",
    "def tiny_vit_5m_224(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(\n",
    "        embed_dims=[64, 128, 160, 320],\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[2, 4, 5, 10],\n",
    "        window_sizes=[7, 7, 14, 7],\n",
    "        drop_path_rate=0.0,\n",
    "    )\n",
    "    model_kwargs.update(kwargs)\n",
    "    return _create_tiny_vit('tiny_vit_5m_224', pretrained, **model_kwargs)\n",
    "\n",
    "\n",
    "def tiny_vit_11m_224(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(\n",
    "        embed_dims=[64, 128, 256, 448],\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[2, 4, 8, 14],\n",
    "        window_sizes=[7, 7, 14, 7],\n",
    "        drop_path_rate=0.1,\n",
    "    )\n",
    "    model_kwargs.update(kwargs)\n",
    "    return _create_tiny_vit('tiny_vit_11m_224', pretrained, **model_kwargs)\n",
    "\n",
    "\n",
    "def tiny_vit_21m_224(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(\n",
    "        embed_dims=[96, 192, 384, 576],\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 18],\n",
    "        window_sizes=[7, 7, 14, 7],\n",
    "        drop_path_rate=0.2,\n",
    "    )\n",
    "    model_kwargs.update(kwargs)\n",
    "    return _create_tiny_vit('tiny_vit_21m_224', pretrained, **model_kwargs)\n",
    "\n",
    "\n",
    "def tiny_vit_21m_384(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(\n",
    "        img_size=384,\n",
    "        embed_dims=[96, 192, 384, 576],\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 18],\n",
    "        window_sizes=[12, 12, 24, 12],\n",
    "        drop_path_rate=0.1,\n",
    "    )\n",
    "    model_kwargs.update(kwargs)\n",
    "    return _create_tiny_vit('tiny_vit_21m_384', pretrained, **model_kwargs)\n",
    "\n",
    "\n",
    "def tiny_vit_21m_512(pretrained=False, **kwargs):\n",
    "    model_kwargs = dict(\n",
    "        img_size=512,\n",
    "        embed_dims=[96, 192, 384, 576],\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 18],\n",
    "        window_sizes=[16, 16, 32, 16],\n",
    "        drop_path_rate=0.1,\n",
    "    )\n",
    "    model_kwargs.update(kwargs)\n",
    "    return _create_tiny_vit('tiny_vit_21m_512', pretrained, **model_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchmetrics.classification import Accuracy, MulticlassF1Score\n",
    "\n",
    "from models.tiny_vit import tiny_vit_21m_224\n",
    "\n",
    "\n",
    "class TinyVitLightning(L.LightningModule):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = tiny_vit_21m_224(\n",
    "            pretrained=config[\"pretrained\"], num_classes=config[\"num_classes\"]\n",
    "        )\n",
    "\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=config[\"num_classes\"])\n",
    "        self.f1_score = MulticlassF1Score(num_classes=config[\"num_classes\"])\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.lr = 2.5e-4\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pretrained_model(x)\n",
    "\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        acc = self.accuracy(preds, y)\n",
    "        f1 = self.f1_score(preds, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_accuracy\", acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_f1\", f1, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        acc = self.accuracy(preds, y)\n",
    "        f1 = self.f1_score(preds, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_f1\", f1, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self) -> Tuple[List[torch.optim.Optimizer], List[torch.optim.lr_scheduler._LRScheduler]]:\n",
    "        optimizer = Adam(self.parameters(), lr=self.lr, weight_decay=1e-8)\n",
    "        scheduler = StepLR(optimizer, step_size=10, gamma=0.01)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "config = {    \n",
    "    \"num_classes\": 137,\n",
    "    \"pretrained\": True,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 10,\n",
    "    }\n",
    "\n",
    "data_module = GeoDataModule(batch_size=config[\"batch_size\"])\n",
    "model = TinyVitLightning(config)\n",
    "trainer = Trainer(\n",
    "    max_epochs=config[\"num_epochs\"],\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")],\n",
    ")\n",
    "trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
